<div align="center">

<h2>Towards Generating Probabilistic Ensembles of 3D Rigid Body Simulations from RGBD Image Observations Using Gaussian Processes</h2>

<div>
  Onur Beker<sup>1</sup>&emsp;
  Yunhan Wang<sup>1</sup>&emsp;
  Barbu Bojor<sup>1</sup>&emsp;
</div>

<div>
    <sup>1</sup>University of Tübingen
</div>


</div>

## Description
This repository contains a pipeline to generate probabilistic ensembles of 3D rigid body simulations from RGBD image observations using Gaussian processes. We also provide the tools to visualize the steps we attempted to build up the intuition. In particular, given 10 RGBD images of an object and its 3D shape, as well as a single RGBD image of another object of a similar category. We would like to reconstruct the latter object's occluded 3D shape (since we cannot observe the 3D shape from one view). 

The standard approach for generating robot behavior involves optimizing the control inputs to a deterministic rigid body simulation. This approach, while remarkably successful in controlled environments like factories, does not account for uncertainty and is therefore poorly suited for uncontrolled environments. One way to address this limitation is to optimize the controls to maximize the total probability of success across a diverse ensemble of rigid body simulations. As a first step towards building such a simulation framework, we propose a method of constructing a probabilistic ensemble of 3D reconstructions of an object given a single RGBD image of it. Our method first constructs a dataset of 3D points that are likely to lie within the object boundaries by retrieving geometric primitives from a category-level mesh template and realigning them with the RGBD image. It then fits a Gaussian process to this data and samples from the resulting posterior. We evaluate our method on a real-world cereal box and show that it can generate a sensible set of 3D reconstructions to build the required ensemble.

## Getting Started 
The file structure is shown as follows:
```
.
├── app
│    ├── bob_scan/
│    ├── oreo_scan/
│    ├── data/
│    ├── build_dpc_cereal_to_cereal_w_gpis.ipynb
│    ├── build_dpc_cereal_to_cereal.ipynb
│    ├── capture_rgbd.ipynb
│    └── gpis_mcubes.ipynb
├── vimex/
├── requirements.txt
└── README.md
```

The main pipeline is contained in a Jupyter notebook: `app/build_dpc_cereal_to_cereal_w_gpis.ipynb`.
In this demo, we demonstrate the effectiveness of our method using real-world cereal boxes (the category). One instance of a cereal box is painted with the photo of "Bob Ross", another instance is an Oreo box. `data/` contains the 3D information (.obj files) and the textures of these two instances. 

Each of `bob_scan/` and `oreo_scan/` contains 10 RGBD images of the respective box, as well as the camera parameters when capturing these images. `capture_rgbd.ipynb` contains the image capture pipeline. `vimex/` is a library for data processing and visualization. `gpis_mcubes.ipynb` contains the pipeline to reconstruct 3D shape from Gaussian Implicit Surface.

### Set up the environment 
Please navigate to this directory. Create a dedicated conda environment for running this project is as follows:
```
conda create --name <env> --file requirements.txt
```

### Run the pipeline
Please arrive at `app/build_dpc_cereal_to_cereal_w_gpis.ipynb`, and change the variable named `VIMEX_BASE_DIR` to the path where this repository is located in your machine. We reconstruct DPC from those object 3D scans. DPC stands for descriptor point cloud, in which each point of the point cloud contains descriptor information generated by [DINOS_v2](https://github.com/facebookresearch/dinov2). 

### Hardware requirement
To process images by DINO_v2, please reserve at least 3GB of virtual memory. Ideally, you would have a GPU that allows CUDA to parallelize the inference of DINO_v2. However, you could also use a CPU (process at lower speed). Please simply replace all text `cuda` in the code with `cpu`.

## How to create your own dataset
You can use objects of other categories for this project. First, we need textured 3D object data, you are free to include object data scanned by yourself or downloaded from [poly](https://poly.cam/explore). 

Then you can place them in `app/data/`. To obtain object RGBD images, you could use `app/capture_rgbd.ipynb`, please first change the `.obj` path to your object, then Open3D will render the 3D object in a new window. You can press `P` to capture RGB images and `D` to capture depth images. The corresponding images and camera parameters will be generated in the current directory. 





